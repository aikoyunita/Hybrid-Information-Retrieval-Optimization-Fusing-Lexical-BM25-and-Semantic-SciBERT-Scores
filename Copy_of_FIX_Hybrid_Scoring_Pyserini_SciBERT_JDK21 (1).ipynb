{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Hybrid Information Retrieval Optimization: Fusing Lexical (BM25) and Semantic (SciBERT) Scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "setup-install",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup-install",
        "outputId": "ca19fae9-a6d4-4c30-da6c-f83b2906d4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Configuring local Java environment for Pyserini ---\n",
            "Set JAVA_HOME to: /usr\n",
            "Set JVM_PATH to: /usr/jre/lib/amd64/server/libjvm.so\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# --- 1. Python Path Setup (Optional, but often helpful) ---\n",
        "# Ensure local packages are on the path, especially if using a virtual environment\n",
        "import sys\n",
        "if not os.path.dirname(os.getcwd()) in sys.path:\n",
        "    sys.path.append(os.path.dirname(os.getcwd()))\n",
        "\n",
        "# --- 2. Java Environment Configuration (CRITICAL for Pyserini) ---\n",
        "print(\"--- Configuring local Java environment for Pyserini ---\")\n",
        "# This block attempts to find your Java installation. \n",
        "# You might need to adjust the path based on where JDK 21 is installed on your OS.\n",
        "\n",
        "# Common check for Mac/Linux\n",
        "try:\n",
        "    if sys.platform.startswith(('linux', 'darwin')):\n",
        "        # Attempt to find the Java path via the system's 'java' command\n",
        "        java_bin_path = subprocess.check_output(['readlink', '-f', '/usr/bin/java']).decode('utf-8').strip()\n",
        "        java_home = os.path.dirname(os.path.dirname(java_bin_path))\n",
        "        os.environ['JAVA_HOME'] = java_home\n",
        "        \n",
        "        # Set JVM_PATH to the libjvm.so file Pyserini needs (might need manual adjustment)\n",
        "        libjvm_path = os.path.join(java_home, 'lib/server/libjvm.so')\n",
        "        if not os.path.exists(libjvm_path):\n",
        "            # Fallback for alternative JDK structures\n",
        "            libjvm_path = os.path.join(java_home, 'jre/lib/amd64/server/libjvm.so')\n",
        "        os.environ['JVM_PATH'] = libjvm_path\n",
        "        print(f\"Set JAVA_HOME to: {os.environ.get('JAVA_HOME')}\")\n",
        "        print(f\"Set JVM_PATH to: {os.environ.get('JVM_PATH')}\")\n",
        "    elif sys.platform.startswith('win'):\n",
        "        # On Windows, you typically set JAVA_HOME in your system environment variables.\n",
        "        # If set, pyjnius will usually find it. If not, uncomment and set manually:\n",
        "        # os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21' \n",
        "        print(\"Assuming JAVA_HOME is set in Windows environment variables.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Automatic Java path detection failed: {e}. Ensure JAVA_HOME is set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "setup-java",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup-java",
        "outputId": "d749e723-aba8-4b7f-bf75-614114108d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Installing Packages ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 2: Manual JDK 21 Configuration (The Final Path) ---\n",
            "âœ… Success! JVM_PATH set to: /Library/Java/JavaVirtualMachines/temurin-21.jdk/Contents/Home/lib/server/libjvm.dylib\n",
            "JAVA_HOME set to: /Library/Java/JavaVirtualMachines/temurin-21.jdk/Contents/Home\n",
            "java -version -> openjdk version \"21.0.2\" 2024-01-16 LTS\n",
            "--- Java setup complete. Proceeding to imports... ---\n",
            "\n",
            "--- Step 3: Importing Libraries ---\n",
            "âœ… Libraries imported successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"--- Step 1: Installing Packages ---\")\n",
        "# Force reinstall/check packages in the notebook environment\n",
        "print(subprocess.run(['pip', 'install', '-q', 'pyserini', 'pyjnius', 'transformers>=4.40', 'sentence-transformers>=2.7', 'torch'], capture_output=True, text=True).stdout)\n",
        "\n",
        "\n",
        "print(\"--- Step 2: Manual JDK 21 Configuration (The Final Path) ---\")\n",
        "\n",
        "# The confirmed JDK 21 folder on your system\n",
        "JAVA_HOME_PATH = '/Library/Java/JavaVirtualMachines/temurin-21.jdk/Contents/Home'\n",
        "os.environ['JAVA_HOME'] = JAVA_HOME_PATH\n",
        "\n",
        "# We use the final, most likely path: /lib/server/libjvm.dylib\n",
        "JVM_PATH_ABS = os.path.join(JAVA_HOME_PATH, 'lib/server/libjvm.dylib')\n",
        "os.environ['JVM_PATH'] = JVM_PATH_ABS\n",
        "\n",
        "\n",
        "# Verification Check\n",
        "if os.path.exists(os.environ['JVM_PATH']):\n",
        "    print(f\"âœ… Success! JVM_PATH set to: {os.environ['JVM_PATH']}\")\n",
        "    print(f\"JAVA_HOME set to: {os.environ['JAVA_HOME']}\")\n",
        "    # Optional: Verify Java version (can be tricky on Mac)\n",
        "    try:\n",
        "        os.environ['PATH'] = os.path.join(JAVA_HOME_PATH, 'bin') + ':' + os.environ['PATH']\n",
        "        java_version_output = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT).decode().split('\\n')[0]\n",
        "        print('java -version ->', java_version_output)\n",
        "    except Exception:\n",
        "        print(\"Warning: Could not run 'java -version', but JVM path is confirmed.\")\n",
        "    print(\"--- Java setup complete. Proceeding to imports... ---\")\n",
        "else:\n",
        "    # If this STILL fails, the file extension or the jre path is the issue.\n",
        "    # We will try the next most likely path (with the 'jre' folder)\n",
        "    JVM_PATH_ABS_JRC = os.path.join(JAVA_HOME_PATH, 'jre/lib/server/libjvm.dylib')\n",
        "    if os.path.exists(JVM_PATH_ABS_JRC):\n",
        "        os.environ['JVM_PATH'] = JVM_PATH_ABS_JRC\n",
        "        print(f\"âœ… Success! JVM_PATH corrected to: {os.environ['JVM_PATH']}\")\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            f\"CRITICAL ERROR: The file was not found at ANY standard path. \"\n",
        "            f\"Checked: {JVM_PATH_ABS} and {JVM_PATH_ABS_JRC}. \"\n",
        "            \"The internal structure of your Temurin installation is highly customized.\"\n",
        "        )\n",
        "\n",
        "# --- This section is where the import would happen in the next cell ---\n",
        "print(\"\\n--- Step 3: Importing Libraries ---\")\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "print(\"âœ… Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ihf71EsShPY9",
      "metadata": {
        "id": "ihf71EsShPY9"
      },
      "outputs": [],
      "source": [
        "from pyserini.search.lucene import LuceneSearcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "verify-imports",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify-imports",
        "outputId": "47317c8b-25f8-4778-fe3d-ea628dbbc207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pyserini: 1.2.0\n",
            "transformers: 4.56.2\n",
            "sentence_transformers: 5.1.1\n",
            "torch: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import importlib.metadata as md\n",
        "import pyserini, transformers, sentence_transformers, torch\n",
        "\n",
        "def show(mod, pkg_name=None):\n",
        "    pkg = pkg_name or mod.__name__.replace('_','-')\n",
        "    try:\n",
        "        ver = mod.__version__\n",
        "    except Exception:\n",
        "        try:\n",
        "            ver = md.version(pkg)\n",
        "        except Exception:\n",
        "            ver = 'installed'\n",
        "    print(f\"{mod.__name__}:\", ver)\n",
        "\n",
        "show(pyserini, 'pyserini')\n",
        "show(transformers, 'transformers')\n",
        "show(sentence_transformers, 'sentence-transformers')\n",
        "show(torch, 'torch')\n",
        "\n",
        "# Import Lucene-backed searcher now that JVM is configured\n",
        "from pyserini.search.lucene import LuceneSearcher\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "corpus-md",
      "metadata": {
        "id": "corpus-md"
      },
      "source": [
        "## Create a tiny demo corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "corpus-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "corpus-code",
        "outputId": "76b4f0c8-95f3-49ac-cbaa-76498dc9455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Corpus successfully created at ./data/corpus-jsonl/corpus.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "\n",
        "# Define a local, relative path (this will create a 'data' folder next to your notebook)\n",
        "CORPUS_DIR = './data/corpus-jsonl' \n",
        "os.makedirs(CORPUS_DIR, exist_ok=True)\n",
        "\n",
        "docs = [\n",
        "  {\"id\": \"D1\", \"title\": \"ACE2 expression in smokers\", \"contents\": \"Several studies report differential ACE2 receptor expression in smokers versus non-smokers, which could influence viral entry dynamics.\"},\n",
        "  {\"id\": \"D2\", \"title\": \"Long COVID symptoms in adolescents\", \"contents\": \"Adolescents may experience fatigue, headaches, sleep disturbance, and concentration difficulties as part of long COVID.\"},\n",
        "  {\"id\": \"D3\", \"title\": \"mRNA vaccine efficacy in elderly\", \"contents\": \"Effectiveness of mRNA vaccines against severe disease remains high in older adults, though waning immunity is a factor.\"},\n",
        "  {\"id\": \"D4\", \"title\": \"T-cell response to Omicron variant\", \"contents\": \"Studies indicate that prior infection and vaccination generate T-cells that cross-react with the Omicron spike protein, offering protection against severe outcomes.\"},\n",
        "  {\"id\": \"D5\", \"title\": \"Aerosol transmission dynamics\", \"contents\": \"Transmission risk is strongly influenced by ventilation, occupancy, and exposure time, supporting aerosol spread as a key factor.\"},\n",
        "  {\"id\": \"D6\", \"title\": \"Monoclonal antibody resistance\", \"contents\": \"The continuous evolution of the virus, particularly new variants, has led to resistance to several therapeutic monoclonal antibodies.\"},\n",
        "  {\"id\": \"D7\", \"title\": \"Vitamin D levels and severity\", \"contents\": \"Low Vitamin D levels are consistently associated with increased risk of severe COVID-19 and mortality.\"},\n",
        "  {\"id\": \"D8\", \"title\": \"Smoking and respiratory epithelium\", \"contents\": \"Chronic smoking alters ciliary beat frequency and epithelial integrity in the respiratory tract.\"}\n",
        "]\n",
        "\n",
        "# Write the documents to the new local path\n",
        "with open(os.path.join(CORPUS_DIR, 'corpus.jsonl'), 'w', encoding='utf-8') as f:\n",
        "    for doc in docs:\n",
        "        json_doc = json.dumps(doc)\n",
        "        f.write(json_doc + '\\n')\n",
        "        \n",
        "print(f\"âœ… Corpus successfully created at {os.path.join(CORPUS_DIR, 'corpus.jsonl')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "index-md",
      "metadata": {
        "id": "index-md"
      },
      "source": [
        "## Build a BM25 index with Pyserini (Lucene)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "index-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "index-code",
        "outputId": "188ac1df-caf0-4cd5-d8a0-79e9b5f533a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "WARNING: Using incubator modules: jdk.incubator.vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-28 23:24:43,574 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:205) - Setting log level to INFO\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:208) - ============ Loading Index Configuration ============\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:209) - AbstractIndexer settings:\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:210) -  + DocumentCollection path: ./data/corpus-jsonl\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:211) -  + CollectionClass: JsonCollection\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:212) -  + Index path: ./data/index\n",
            "2025-09-28 23:24:43,577 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:213) -  + Threads: 2\n",
            "2025-09-28 23:24:43,578 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:214) -  + Optimize (merge segments)? false\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sep 28, 2025 11:24:43 PM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
            "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-28 23:24:43,608 INFO  [main] index.IndexCollection (IndexCollection.java:246) - Using DefaultEnglishAnalyzer\n",
            "2025-09-28 23:24:43,608 INFO  [main] index.IndexCollection (IndexCollection.java:247) - Stemmer: porter\n",
            "2025-09-28 23:24:43,608 INFO  [main] index.IndexCollection (IndexCollection.java:248) - Keep stopwords? false\n",
            "2025-09-28 23:24:43,609 INFO  [main] index.IndexCollection (IndexCollection.java:249) - Stopwords file: null\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:197) - IndexCollection settings:\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:198) -  + Generator: DefaultLuceneDocumentGenerator\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:199) -  + Language: en\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:200) -  + Stemmer: porter\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:201) -  + Keep stopwords? false\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:202) -  + Stopwords: null\n",
            "2025-09-28 23:24:43,720 INFO  [main] index.IndexCollection (IndexCollection.java:203) -  + Store positions? true\n",
            "2025-09-28 23:24:43,721 INFO  [main] index.IndexCollection (IndexCollection.java:204) -  + Store docvectors? true\n",
            "2025-09-28 23:24:43,721 INFO  [main] index.IndexCollection (IndexCollection.java:205) -  + Store document \"contents\" field? false\n",
            "2025-09-28 23:24:43,721 INFO  [main] index.IndexCollection (IndexCollection.java:206) -  + Store document \"raw\" field? true\n",
            "2025-09-28 23:24:43,721 INFO  [main] index.IndexCollection (IndexCollection.java:207) -  + Additional fields to index: []\n",
            "2025-09-28 23:24:43,721 INFO  [main] index.IndexCollection (IndexCollection.java:208) -  + Whitelist: null\n",
            "2025-09-28 23:24:43,722 INFO  [main] index.IndexCollection (IndexCollection.java:209) -  + Pretokenized?: false\n",
            "2025-09-28 23:24:43,722 INFO  [main] index.IndexCollection (IndexCollection.java:210) -  + Codec: Lucene99\n",
            "2025-09-28 23:24:43,722 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:238) - ============ Indexing Collection ============\n",
            "2025-09-28 23:24:43,724 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:247) - Thread pool with 2 threads initialized.\n",
            "2025-09-28 23:24:43,724 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:248) - 1 file found in ./data/corpus-jsonl\n",
            "2025-09-28 23:24:43,724 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:249) - Starting to index...\n",
            "2025-09-28 23:24:43,860 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:307) - Indexing Complete! 8 documents indexed\n",
            "2025-09-28 23:24:43,860 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:308) - ============ Final Counter Values ============\n",
            "2025-09-28 23:24:43,861 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:309) - indexed:                8\n",
            "2025-09-28 23:24:43,861 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:310) - unindexable:            0\n",
            "2025-09-28 23:24:43,862 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:311) - empty:                  0\n",
            "2025-09-28 23:24:43,862 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:312) - skipped:                0\n",
            "2025-09-28 23:24:43,862 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:313) - errors:                 0\n",
            "2025-09-28 23:24:43,866 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:316) - Total 8 documents indexed in 00:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "CORPUS_DIR=\"./data/corpus-jsonl\"\n",
        "INDEX_DIR=\"./data/index\"\n",
        "\n",
        "python -m pyserini.index.lucene \\\n",
        "  --collection JsonCollection \\\n",
        "  --input ${CORPUS_DIR} \\\n",
        "  --index ${INDEX_DIR} \\\n",
        "  --generator DefaultLuceneDocumentGenerator \\\n",
        "  --threads 2 --storePositions --storeDocvectors --storeRaw"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hybrid-md",
      "metadata": {
        "id": "hybrid-md"
      },
      "source": [
        "## Hybrid scoring utilities (BM25 â†’ Crossâ€‘Encoder â†’ Fusion)\n",
        "We define: `bm25_search`, `cross_encoder_rerank`, normalizers, **RRF**, and a `fuse_scores` helper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "hybrid-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hybrid-code",
        "outputId": "26a11349-6364-44d2-ba05-1422c19304be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Defining hybrid scoring functions ---\n"
          ]
        }
      ],
      "source": [
        "# 5. Define the hybrid scoring functions (from 'hybrid-code')\n",
        "# This must be the absolute first executable line for Python 3.x\n",
        "from __future__ import annotations\n",
        "\n",
        "print(\"--- Defining hybrid scoring functions ---\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "import json, math\n",
        "import numpy as np\n",
        "\n",
        "# Pyserini and Hugging Face imports\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievedDoc:\n",
        "    docid: str\n",
        "    score: float\n",
        "    title: str | None\n",
        "    contents: str\n",
        "\n",
        "def bm25_search(index_dir: str, query: str, k: int) -> List[RetrievedDoc]:\n",
        "    searcher = LuceneSearcher(index_dir)\n",
        "    hits = searcher.search(query, k=k)\n",
        "    out: List[RetrievedDoc] = []\n",
        "    for h in hits:\n",
        "        raw = searcher.doc(h.docid).raw()\n",
        "        title, contents = None, None\n",
        "        try:\n",
        "            obj = json.loads(raw)\n",
        "            contents = obj.get(\"contents\") or obj.get(\"text\") or raw\n",
        "            title = obj.get(\"title\")\n",
        "        except Exception:\n",
        "            contents = raw\n",
        "        out.append(RetrievedDoc(docid=h.docid, score=h.score, title=title, contents=contents))\n",
        "    return out\n",
        "\n",
        "def cross_encoder_rerank(model_name: str, query: str, docs: List[RetrievedDoc], batch_size: int = 32) -> Dict[str, float]:\n",
        "    ce = CrossEncoder(model_name, max_length=512)\n",
        "    pairs = [[query, d.contents if isinstance(d.contents, str) else str(d.contents)] for d in docs]\n",
        "    # The original notebook runs with no_grad, which is typical for inference, but the library handles it.\n",
        "    scores = ce.predict(pairs, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return {d.docid: float(s) for d, s in zip(docs, scores)}\n",
        "\n",
        "def _zscore(x: np.ndarray) -> np.ndarray:\n",
        "    mu, sd = x.mean(), x.std()\n",
        "    return (x - mu) / (sd + 1e-8)\n",
        "\n",
        "def _minmax(x: np.ndarray) -> np.ndarray:\n",
        "    lo, hi = x.min(), x.max()\n",
        "    if math.isclose(hi, lo):\n",
        "        return np.zeros_like(x)\n",
        "    return (x - lo) / (hi - lo)\n",
        "\n",
        "def normalize_scores(scores: Dict[str, float], method: str) -> Dict[str, float]:\n",
        "    keys = list(scores.keys())\n",
        "    arr = np.array([scores[k] for k in keys], dtype=float)\n",
        "    if method == 'zscore':\n",
        "        arr = _zscore(arr)\n",
        "    elif method == 'minmax':\n",
        "        arr = _minmax(arr)\n",
        "    else:\n",
        "        raise ValueError(f'Unknown normalization method: {method}')\n",
        "    return {k: float(v) for k, v in zip(keys, arr)}\n",
        "\n",
        "def reciprocal_rank_fusion(rankings: List[List[str]], k: int = 60) -> Dict[str, float]:\n",
        "    fused: Dict[str, float] = {}\n",
        "    for ranking in rankings:\n",
        "        for rank, docid in enumerate(ranking):\n",
        "            fused[docid] = fused.get(docid, 0.0) + 1.0 / (k + rank + 1)\n",
        "    return fused\n",
        "\n",
        "def fuse_scores(bm25_scores: Dict[str, float], ce_scores: Dict[str, float], *, method: str = 'weighted', norm: str = 'zscore', alpha: float = 0.5) -> Dict[str, float]:\n",
        "    if method == 'rrf':\n",
        "        bm25_ranking = [d for d, _ in sorted(bm25_scores.items(), key=lambda x: -x[1])]\n",
        "        ce_ranking = [d for d, _ in sorted(ce_scores.items(), key=lambda x: -x[1])]\\\n",
        "        # For RRF, we only fuse the documents that are retrieved by BM25 (the input to the CE re-ranker)\n",
        "        # However, the current implementation uses *all* documents from bm25_scores/ce_scores, which should be the same set.\n",
        "        return reciprocal_rank_fusion([bm25_ranking, ce_ranking])\n",
        "\n",
        "    # weighted fusion (normalize + sum)\n",
        "    all_ids = list(set(bm25_scores) | set(ce_scores))\n",
        "    bm = {d: bm25_scores.get(d, float('-inf')) for d in all_ids}\n",
        "    ce = {d: ce_scores.get(d, float('-inf')) for d in all_ids}\n",
        "    def safe_replace_missing(s: Dict[str, float]) -> Dict[str, float]:\n",
        "        vals = [v for v in s.values() if v != float('-inf')]\n",
        "        if not vals:\n",
        "            return {k: 0.0 for k in s}\n",
        "        floor = min(vals) - 1.0\n",
        "        return {k: (floor if v == float('-inf') else v) for k, v in s.items()}\n",
        "    bm = safe_replace_missing(bm)\n",
        "    ce = safe_replace_missing(ce)\n",
        "    bm_norm = normalize_scores(bm, norm)\n",
        "    ce_norm = normalize_scores(ce, norm)\n",
        "    return {d: alpha * bm_norm[d] + (1.0 - alpha) * ce_norm[d] for d in all_ids}\n",
        "\n",
        "def pretty_print(name: str, ranking: List[Tuple[str, float]], lookup: Dict[str, RetrievedDoc], topn: int = 5):\n",
        "    print(f\"\\n=== {name} Top-{topn} ===\")\n",
        "    for i, (docid, score) in enumerate(ranking[:topn], start=1):\n",
        "        rec = lookup[docid]\n",
        "        title = f\"{rec.title} â€” \" if rec.title else \"\"\n",
        "        snippet = (rec.contents or '').replace('\\n', ' ')\n",
        "        if len(snippet) > 220:\n",
        "            snippet = snippet[:220] + 'â€¦'\n",
        "        print(f\"{i:>2}. {docid}  (score={score:.4f})\")\n",
        "        print(f\"    {title}{snippet}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-md",
      "metadata": {
        "id": "run-md"
      },
      "source": [
        "## Run the hybrid scorer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "run-code",
      "metadata": {
        "id": "run-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running final comparative hybrid scorer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BM25 (Lexical) Top-5 ===\n",
            " 1. D2  (score=2.6194)\n",
            "    Long COVID symptoms in adolescents â€” Adolescents may experience fatigue, headaches, sleep disturbance, and concentration difficulties as part of long COVID.\n",
            " 2. D7  (score=0.6898)\n",
            "    Vitamin D levels and severity â€” Low Vitamin D levels are consistently associated with increased risk of severe COVID-19 and mortality.\n",
            "\n",
            "=== Cross-Encoder (SciBERT) Top-5 ===\n",
            " 1. D2  (score=0.4665)\n",
            "    Long COVID symptoms in adolescents â€” Adolescents may experience fatigue, headaches, sleep disturbance, and concentration difficulties as part of long COVID.\n",
            " 2. D7  (score=0.4150)\n",
            "    Vitamin D levels and severity â€” Low Vitamin D levels are consistently associated with increased risk of severe COVID-19 and mortality.\n",
            "\n",
            "=== WSS FUSED (Weighted Sum, alpha=0.5) Top-5 ===\n",
            " 1. D2  (score=1.0000)\n",
            "    Long COVID symptoms in adolescents â€” Adolescents may experience fatigue, headaches, sleep disturbance, and concentration difficulties as part of long COVID.\n",
            " 2. D7  (score=-1.0000)\n",
            "    Vitamin D levels and severity â€” Low Vitamin D levels are consistently associated with increased risk of severe COVID-19 and mortality.\n",
            "\n",
            "--- Comparative Analysis: Reciprocal Rank Fusion (RRF) ---\n",
            "\n",
            "=== RRF FUSION (k=60) Top-5 ===\n",
            " 1. D2  (score=0.0328)\n",
            "    Long COVID symptoms in adolescents â€” Adolescents may experience fatigue, headaches, sleep disturbance, and concentration difficulties as part of long COVID.\n",
            " 2. D7  (score=0.0323)\n",
            "    Vitamin D levels and severity â€” Low Vitamin D levels are consistently associated with increased risk of severe COVID-19 and mortality.\n"
          ]
        }
      ],
      "source": [
        "# 6. Execute the final run logic (This cell will now use SciBERT and compare WSS vs RRF)\n",
        "\n",
        "print(\"--- Running final comparative hybrid scorer ---\")\n",
        "# ðŸ›‘ CORRECTED PATH: Use the local index created in the Bash cell ðŸ›‘\n",
        "INDEX_DIR = './data/index' \n",
        "QUERY = 'long COVID symptoms in adolescents'\n",
        "K = 8 # Total documents retrieved\n",
        "TOPN = 5 # Number of top results to print\n",
        "\n",
        "# *** USE YOUR INTENDED SciBERT MODEL NAME HERE ***\n",
        "# This model will now download successfully because you ran 'huggingface-cli login'\n",
        "CE_MODEL = 'allenai/scibert_scivocab_uncased'\n",
        "\n",
        "\n",
        "FUSION = 'weighted'\n",
        "NORM = 'zscore'\n",
        "ALPHA = 0.5\n",
        "\n",
        "# Stage 1 & 2 Execution\n",
        "bm25_docs = bm25_search(INDEX_DIR, QUERY, k=K)\n",
        "id2doc = {d.docid: d for d in bm25_docs}\n",
        "bm25_scores = {d.docid: d.score for d in bm25_docs}\n",
        "ce_scores = cross_encoder_rerank(CE_MODEL, QUERY, bm25_docs, batch_size=16)\n",
        "\n",
        "# Rankings for the report\n",
        "bm25_ranking = sorted(bm25_scores.items(), key=lambda x: -x[1])\n",
        "ce_ranking = sorted(ce_scores.items(), key=lambda x: -x[1])\n",
        "\n",
        "# 1. Weighted Sum Fusion (WSS) - Your primary method\n",
        "fused_scores = fuse_scores(bm25_scores, ce_scores, method=FUSION, norm=NORM, alpha=ALPHA)\n",
        "fused_ranking = sorted(fused_scores.items(), key=lambda x: -x[1])\n",
        "\n",
        "# Print All Rankings\n",
        "pretty_print('BM25 (Lexical)', bm25_ranking, id2doc, topn=TOPN)\n",
        "pretty_print('Cross-Encoder (SciBERT)', ce_ranking, id2doc, topn=TOPN)\n",
        "pretty_print(f'WSS FUSED (Weighted Sum, alpha={ALPHA})', fused_ranking, id2doc, topn=TOPN)\n",
        "\n",
        "# 2. Reciprocal Rank Fusion (RRF) - Your comparative baseline\n",
        "print(\"\\n--- Comparative Analysis: Reciprocal Rank Fusion (RRF) ---\")\n",
        "rrf_scores = fuse_scores(bm25_scores, ce_scores, method='rrf')\n",
        "rrf_ranking = sorted(rrf_scores.items(), key=lambda x: -x[1])\n",
        "pretty_print('RRF FUSION (k=60)', rrf_ranking, id2doc, topn=TOPN)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
